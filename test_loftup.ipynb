{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsamplers import load_loftup_checkpoint, norm, unnorm\n",
    "from featurizers import get_featurizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage import color\n",
    "from skimage.color import lch2lab, lab2rgb\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "rr.init(\"loftup_comparison\", recording_id=\"loftup_comparison\")\n",
    "current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "rr.save(f\"/root/repos/vlmaps/data/allmend_trail_recording_2025_12_06_full_trail_zed_sdk/rerun/loftup/loftup_comparison_{current_time}.rrd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPCA(object):\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean_ = X.mean(dim=0)\n",
    "        unbiased = X - self.mean_.unsqueeze(0)\n",
    "        U, S, V = torch.pca_lowrank(unbiased, q=self.n_components, center=False, niter=4)\n",
    "        self.components_ = V.T\n",
    "        self.singular_values_ = S\n",
    "\n",
    "        ## --- FIX STARTS HERE --- ##\n",
    "        # Enforce a deterministic sign for each component to prevent color flipping.\n",
    "        for i in range(self.n_components):\n",
    "            # Find the element with the largest absolute value in the component vector\n",
    "            max_abs_idx = torch.argmax(torch.abs(self.components_[i]))\n",
    "            # If that element is negative, flip the entire component vector\n",
    "            if self.components_[i, max_abs_idx] < 0:\n",
    "                self.components_[i] *= -1\n",
    "        ## --- FIX ENDS HERE --- ##\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        t0 = X - self.mean_.unsqueeze(0)\n",
    "        projected = t0 @ self.components_.T\n",
    "        return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(tensor, feature_range):\n",
    "    \"\"\"Scales a tensor to a given feature range.\"\"\"\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    if tensor_max == tensor_min:\n",
    "        return torch.full_like(tensor, (feature_range[0] + feature_range[1]) / 2)\n",
    "    scaled = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "    return scaled * (feature_range[1] - feature_range[0]) + feature_range[0]\n",
    "\n",
    "def pca(image_feats_list, dim=3, fit_pca=None, use_torch_pca=True, max_samples=None, use_lch=True):\n",
    "    device = image_feats_list[0].device\n",
    "\n",
    "    def flatten(tensor, target_size=None):\n",
    "        if len(tensor.shape) == 2:\n",
    "            return tensor.detach().cpu()\n",
    "        if target_size is not None and fit_pca is None:\n",
    "            tensor = F.interpolate(tensor, (target_size, target_size), mode=\"bilinear\")\n",
    "        B, C, H, W = tensor.shape\n",
    "        return tensor.permute(1, 0, 2, 3).reshape(C, B * H * W).permute(1, 0).detach().cpu()\n",
    "\n",
    "    if len(image_feats_list) > 1 and fit_pca is None:\n",
    "        if len(image_feats_list[0].shape) == 2:\n",
    "            target_size = None\n",
    "        else:\n",
    "            target_size = image_feats_list[0].shape[2]\n",
    "    else:\n",
    "        target_size = None\n",
    "\n",
    "    flattened_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        flattened_feats.append(flatten(feats, target_size))\n",
    "    x = torch.cat(flattened_feats, dim=0)\n",
    "\n",
    "    # Subsample the data if max_samples is set and the number of samples exceeds max_samples\n",
    "    if max_samples is not None and x.shape[0] > max_samples:\n",
    "        indices = torch.randperm(x.shape[0])[:max_samples]\n",
    "        x = x[indices]\n",
    "\n",
    "    if fit_pca is None:\n",
    "        if use_torch_pca:\n",
    "            fit_pca = TorchPCA(n_components=dim).fit(x)\n",
    "        else:\n",
    "            fit_pca = PCA(n_components=dim).fit(x)\n",
    "\n",
    "    reduced_feats = []\n",
    "    for feats in image_feats_list:\n",
    "        x_red = fit_pca.transform(flatten(feats))\n",
    "        if isinstance(x_red, np.ndarray):\n",
    "            x_red = torch.from_numpy(x_red).float()\n",
    "\n",
    "        if len(feats.shape) == 2:\n",
    "            # For 1D features like CLS token, standard normalization is fine\n",
    "            x_red -= x_red.min(dim=0, keepdim=True).values\n",
    "            x_red /= x_red.max(dim=0, keepdim=True).values\n",
    "            reduced_feats.append(x_red)\n",
    "            continue\n",
    "\n",
    "        # For 2D spatial features\n",
    "        B, C, H, W = feats.shape\n",
    "        \n",
    "        if use_lch:\n",
    "            # 1. Map each PCA component to an LCh channel by scaling to its valid range\n",
    "            # PC1 -> Lightness (L): range [0, 100]\n",
    "            l_channel = min_max_scale(x_red[:, 0], feature_range=(0, 100))\n",
    "            # PC2 -> Chroma (C): range [0, 100] (practical range for colorfulness)\n",
    "            c_channel = min_max_scale(x_red[:, 1], feature_range=(0, 100))\n",
    "            # PC3 -> Hue (h): range [0, 360] (angle for color)\n",
    "            h_channel = min_max_scale(x_red[:, 2], feature_range=(0, 360))\n",
    "\n",
    "            # 2. Stack channels and reshape to image dimensions\n",
    "            lch_image = torch.stack([l_channel, c_channel, h_channel], dim=-1)\n",
    "            lch_image_np = lch_image.reshape(B, H, W, dim).cpu().numpy().squeeze() # Remove batch dim\n",
    "\n",
    "            # 3. Convert LCh to RGB\n",
    "            lab_image_np = lch2lab(lch_image_np)\n",
    "            rgb_image_np = lab2rgb(lab_image_np)\n",
    "\n",
    "            # 4. Convert back to a PyTorch tensor in [B, C, H, W] format\n",
    "            final_image = torch.from_numpy(rgb_image_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        else: # Original RGB mapping\n",
    "            x_red -= x_red.min(dim=0, keepdim=True).values\n",
    "            x_red /= x_red.max(dim=0, keepdim=True).values\n",
    "            final_image = x_red.reshape(B, H, W, dim).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        reduced_feats.append(final_image)\n",
    "\n",
    "    return reduced_feats, fit_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(features, kernel_size=3, sigma=1):\n",
    "    # features: [B, C, H, W]\n",
    "    B, C, H, W = features.shape\n",
    "    # Create Gaussian kernel\n",
    "    import math\n",
    "    def get_gaussian_kernel(kernel_size, sigma):\n",
    "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.)\n",
    "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "        return kernel\n",
    "\n",
    "    kernel = get_gaussian_kernel(kernel_size, sigma).to(features.device)\n",
    "    kernel = kernel.expand(C, 1, kernel_size, kernel_size)\n",
    "    # Pad input\n",
    "    padding = kernel_size // 2\n",
    "    blurred = F.conv2d(features, kernel, padding=padding, groups=C)\n",
    "    return blurred\n",
    "\n",
    "def blend_features(features, kernel_size=3, sigma=1):\n",
    "    blurred = gaussian_blur(features, kernel_size, sigma)\n",
    "    return 0.5 * features + 0.5 * blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_name=\"dinov2s_reg\"):\n",
    "    featurizer_class = base_name\n",
    "    model, patch_size, dim = get_featurizer(featurizer_class)\n",
    "    model = model.to('cuda').eval()\n",
    "    kernel_size = patch_size \n",
    "    lr_size = 224 // patch_size\n",
    "    load_size = 224\n",
    "\n",
    "    torch_hub_name = f\"loftup_{base_name}\"\n",
    "    upsampler = torch.hub.load('andrehuang/loftup', torch_hub_name, pretrained=True)\n",
    "    upsampler = upsampler.to('cuda')\n",
    "\n",
    "    return model, upsampler, load_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"dinov2s_reg\"] # \"dinov2s_reg\", \"dinov2b_reg\", \"clip\", \"siglip2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_sliding_window(\n",
    "    model,\n",
    "    upsampler,\n",
    "    img_path,\n",
    "    crop_size=224,\n",
    "    stride_rate=2/3,\n",
    "    batch_size=8,\n",
    "    max_image_size=518,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a model on a high-resolution image by processing it in overlapping\n",
    "    patches and averaging the results, with memory optimization.\n",
    "\n",
    "    Args:\n",
    "        model: The feature extraction model (e.g., ViT).\n",
    "        upsampler: The model used to upsample low-resolution features.\n",
    "        img_path (str): Path to the high-resolution input image.\n",
    "        crop_size (int): The input size required by the model.\n",
    "        stride_rate (float): The overlap between patches. 2/3 means 1/3 overlap.\n",
    "        batch_size (int): The number of patches to process in a single batch to control VRAM usage.\n",
    "        max_image_size (int, optional): If set, resizes the image's longest side to this value\n",
    "                                        before processing. Defaults to None.\n",
    "        device (str): The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The final high-resolution feature map for the entire image.\n",
    "        torch.Tensor: The normalized high-resolution image tensor.\n",
    "    \"\"\"\n",
    "    # 1. Load and normalize the high-resolution image\n",
    "    # norm = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform = T.Compose([T.ToTensor(), norm])\n",
    "    \n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # --- NEW: Optional image resizing to reduce total number of patches ---\n",
    "    if max_image_size is not None:\n",
    "        original_size = img.size\n",
    "        if max(original_size) > max_image_size:\n",
    "            # Calculate new size while preserving aspect ratio\n",
    "            if original_size[0] > original_size[1]: # Landscape\n",
    "                new_w = max_image_size\n",
    "                new_h = int(max_image_size * original_size[1] / original_size[0])\n",
    "            else: # Portrait or square\n",
    "                new_h = max_image_size\n",
    "                new_w = int(max_image_size * original_size[0] / original_size[1])\n",
    "            \n",
    "            print(f\"Image resized from {original_size} to ({new_w}, {new_h})\")\n",
    "            img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    _, _, h, w = img_tensor.shape\n",
    "\n",
    "    # 2. Calculate patch grid and stride\n",
    "    stride = int(crop_size * stride_rate)\n",
    "    h_grids = int(math.ceil(1.0 * (h - crop_size) / stride)) + 1\n",
    "    w_grids = int(math.ceil(1.0 * (w - crop_size) / stride)) + 1\n",
    "\n",
    "    # 3. Create lists to hold patches and their positions\n",
    "    crops = []\n",
    "    positions = []\n",
    "    for i in range(h_grids):\n",
    "        for j in range(w_grids):\n",
    "            h0 = i * stride\n",
    "            w0 = j * stride\n",
    "            h1 = min(h0 + crop_size, h)\n",
    "            w1 = min(w0 + crop_size, w)\n",
    "            \n",
    "            crop = img_tensor[:, :, h0:h1, w0:w1]\n",
    "            padded_crop = F.pad(crop, (0, crop_size - (w1 - w0), 0, crop_size - (h1 - h0)))\n",
    "            \n",
    "            crops.append(padded_crop)\n",
    "            positions.append((h0, h1, w0, w1))\n",
    "\n",
    "    print(\"h_grids, w_grids:\", h_grids, w_grids)\n",
    "\n",
    "    # 4. --- NEW: Process crops in mini-batches to conserve memory ---\n",
    "    hr_feats_list = []\n",
    "    for i in range(0, len(crops), batch_size):\n",
    "        # Get a mini-batch of crops\n",
    "        batch_crops = torch.cat(crops[i:i + batch_size], dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lr_feats_batch = model(batch_crops)\n",
    "            hr_feats_batch = upsampler(lr_feats_batch, batch_crops)\n",
    "        \n",
    "        # Move results to CPU to free up VRAM for the next batch\n",
    "        hr_feats_list.append(hr_feats_batch.cpu())\n",
    "\n",
    "    # Concatenate all results from mini-batches and move back to target device\n",
    "    hr_feats_batch = torch.cat(hr_feats_list, dim=0).to(device)\n",
    "\n",
    "    # 5. Stitch the results back together by averaging overlaps\n",
    "    feat_dim = hr_feats_batch.shape[1]\n",
    "    final_feats = torch.zeros(1, feat_dim, h, w, device=device)\n",
    "    count_norm = torch.zeros(1, 1, h, w, device=device)\n",
    "\n",
    "    for i, (h0, h1, w0, w1) in enumerate(positions):\n",
    "        feat_patch = hr_feats_batch[i].unsqueeze(0)\n",
    "        unpadded_feat_patch = feat_patch[:, :, :h1-h0, :w1-w0]\n",
    "        final_feats[:, :, h0:h1, w0:w1] += unpadded_feat_patch\n",
    "        count_norm[:, :, h0:h1, w0:w1] += 1\n",
    "\n",
    "    final_feats /= (count_norm + 1e-8)\n",
    "    return final_feats, img_tensor.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, upsampler, img_path, load_size):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(load_size, T.InterpolationMode.BILINEAR),\n",
    "        T.CenterCrop(load_size), # Depending on whether you want a center crop\n",
    "        T.ToTensor(),\n",
    "        norm])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_transformed = transform(img)\n",
    "    normalized_img_tensor = img_transformed.unsqueeze(0).to('cuda')\n",
    "\n",
    "    lr_feats = model(normalized_img_tensor) # 1, dim, lr_size, lr_size\n",
    "    hr_feats = upsampler(lr_feats, normalized_img_tensor) # 1, dim, 224, 224\n",
    "\n",
    "    return hr_feats, img_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_list(frame_ranges):\n",
    "    frames = []\n",
    "    for range_dict in frame_ranges:\n",
    "        start = range_dict[\"start\"]\n",
    "        end = range_dict[\"end\"]\n",
    "        step = range_dict[\"step\"]\n",
    "        frames.extend(range(start, end + 1, step))\n",
    "    return sorted(list(set(frames)))  # Remove duplicates and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ranges = [{\"start\": 400, \"end\": 800, \"step\": 10}]\n",
    "rgb_dir = \"/root/repos/vlmaps/data/allmend_trail_recording_2025_12_06_full_trail_zed_sdk/images\"\n",
    "frame_list = get_frame_list(frame_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /root/.cache/torch/hub/andrehuang_loftup_main\n",
      "/root/.cache/torch/hub/andrehuang_loftup_main/hubconf.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_list:\n",
    "    model, upsampler, load_size = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /root/.cache/torch/hub/andrehuang_loftup_main\n",
      "/root/.cache/torch/hub/andrehuang_loftup_main/hubconf.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
      "Processing frames:   0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   2%|▏         | 1/41 [00:01<01:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   5%|▍         | 2/41 [00:03<01:02,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   7%|▋         | 3/41 [00:04<01:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  10%|▉         | 4/41 [00:06<00:57,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  12%|█▏        | 5/41 [00:07<00:56,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  15%|█▍        | 6/41 [00:09<00:55,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  17%|█▋        | 7/41 [00:11<00:57,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  20%|█▉        | 8/41 [00:13<00:56,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  22%|██▏       | 9/41 [00:14<00:54,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  24%|██▍       | 10/41 [00:16<00:52,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  27%|██▋       | 11/41 [00:18<00:49,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  29%|██▉       | 12/41 [00:19<00:48,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  32%|███▏      | 13/41 [00:21<00:46,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  34%|███▍      | 14/41 [00:23<00:45,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  37%|███▋      | 15/41 [00:24<00:43,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  39%|███▉      | 16/41 [00:26<00:42,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  41%|████▏     | 17/41 [00:28<00:41,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  44%|████▍     | 18/41 [00:30<00:39,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  46%|████▋     | 19/41 [00:32<00:39,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  49%|████▉     | 20/41 [00:33<00:38,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  51%|█████     | 21/41 [00:35<00:36,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  54%|█████▎    | 22/41 [00:37<00:35,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  56%|█████▌    | 23/41 [00:39<00:32,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  59%|█████▊    | 24/41 [00:41<00:31,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  61%|██████    | 25/41 [00:43<00:29,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  63%|██████▎   | 26/41 [00:44<00:27,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  66%|██████▌   | 27/41 [00:46<00:25,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  68%|██████▊   | 28/41 [00:48<00:23,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  71%|███████   | 29/41 [00:50<00:21,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  73%|███████▎  | 30/41 [00:52<00:19,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  76%|███████▌  | 31/41 [00:54<00:18,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  78%|███████▊  | 32/41 [00:55<00:16,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  80%|████████  | 33/41 [00:57<00:14,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  83%|████████▎ | 34/41 [00:59<00:12,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  85%|████████▌ | 35/41 [01:00<00:10,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  88%|████████▊ | 36/41 [01:02<00:08,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  90%|█████████ | 37/41 [01:04<00:06,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  93%|█████████▎| 38/41 [01:05<00:04,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  95%|█████████▌| 39/41 [01:07<00:03,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  98%|█████████▊| 40/41 [01:09<00:01,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resized from (1280, 720) to (518, 291)\n",
      "h_grids, w_grids: 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 41/41 [01:10<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_list:\n",
    "    model, upsampler, load_size = get_model(model_name)\n",
    "    for frame_idx in tqdm(frame_list, desc=\"Processing frames\"):\n",
    "        rr.set_time(timeline=\"frame\", sequence=frame_idx)\n",
    "        img_path = rgb_dir + f\"/{frame_idx:05d}.png\"\n",
    "        \n",
    "        hr_feats, img_transformed = run_model_sliding_window(model, upsampler, img_path, load_size)\n",
    "\n",
    "        blended_feats = blend_features(hr_feats, kernel_size=3, sigma=1)\n",
    "        pca_feats = pca([blended_feats], use_lch=False)[0][0][0]\n",
    "        # pca_feats_cp = cp.fromDlpack(pca_feats.__dlpack__())\n",
    "        # rgb_feats = pca_to_rgb(pca_feats_cp)\n",
    "        # pca_hr_feats = pca([hr_feats])[0][0]\n",
    "        rr.log(f\"img/pca_{model_name}\", rr.Image(pca_feats.permute(1,2,0)))\n",
    "        rr.log(\"img/img\", rr.Image(img_transformed.permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Dinov2 Features of CaT - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to Index SigLip2 Feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS = {\n",
    "    \"road\": (160, 82, 45),  # Saddle Brown\n",
    "    \"vegetation\": (133, 255, 48),  # Green,\n",
    "    \"sky\": (31, 119, 180),  # Steel Blue\n",
    "    \"gravel\": (227, 119, 194),  # Orchid Pink\n",
    "    \"rocks\": (127, 127, 127),  # Gray\n",
    "    \"mud\": (255, 127, 14),  # Dark Orange\n",
    "    \"person\": (23, 190, 207),  # Cyan Blue\n",
    "    \"other\": (158, 218, 229),  # Light Cyan\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_template = \"a photo of a {label}\"\n",
    "text_inputs = [text_template.format(label=label) for label in CLASS_COLORS.keys()]\n",
    "print(text_inputs)\n",
    "text_features = model.forward_text(text_inputs)\n",
    "text_features.shape\n",
    "\n",
    "hr_feats_perm = hr_feats[0].permute(1,2,0)\n",
    "H, W, D = hr_feats_perm.shape\n",
    "hr_feats_flat = hr_feats_perm.reshape(-1, D)\n",
    "\n",
    "scores = hr_feats_flat @ text_features.T\n",
    "class_indices = scores.argmax(axis=1)\n",
    "class_indices = class_indices.reshape(H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"rerun-sdk[notebook]\"\n",
    "import rerun as rr\n",
    "rr.init(\"rerun_example_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mapping = [\n",
    "        rr.AnnotationInfo(id=i, label=label, color=list(CLASS_COLORS[label]))\n",
    "        for i, label in enumerate(CLASS_COLORS.keys())\n",
    "    ]\n",
    "labels_mapping.append(rr.AnnotationInfo(id=len(CLASS_COLORS), label=\"background\", color=[0, 0, 0]))\n",
    "rr.log(\"img\", rr.AnnotationContext(labels_mapping), static=True)\n",
    "rr.log(\"img/seg_image\", rr.SegmentationImage(class_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".loftup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
